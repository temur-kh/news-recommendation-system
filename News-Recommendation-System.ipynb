{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Система для рекомендации новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала считаем все данные датасета, как мы делали в предыдущем ноутбуке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(path):\n",
    "    filenames = [path+pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/news_0283889.json', 'data/news_0175708.json', 'data/news_0194374.json', 'data/news_0007868.json', 'data/news_0122036.json', 'data/news_0254026.json', 'data/news_0103335.json', 'data/news_0119287.json', 'data/news_0033948.json', 'data/news_0241999.json']\n"
     ]
    }
   ],
   "source": [
    "path_to_json = 'data/'\n",
    "json_files = get_filenames(path_to_json)\n",
    "print(json_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'organizations': [],\n",
       " 'uuid': '7de061b1f81736fc573dad0938918666b594ee7a',\n",
       " 'thread': {'social': {'gplus': {'shares': 0},\n",
       "   'pinterest': {'shares': 0},\n",
       "   'vk': {'shares': 0},\n",
       "   'linkedin': {'shares': 0},\n",
       "   'facebook': {'likes': 0, 'shares': 0, 'comments': 0},\n",
       "   'stumbledupon': {'shares': 0}},\n",
       "  'site_full': 'finance.bigmir.net',\n",
       "  'main_image': 'http://bm.img.com.ua/berlin/storage/finance/orig/8/3a/2e6a5dd12c1deb9d1ca50615a40ae3a8.jpg',\n",
       "  'site_section': 'http://finance.bigmir.net/finance.bigmir.net?_ctr=rss',\n",
       "  'section_title': 'Финансы',\n",
       "  'url': 'http://finance.bigmir.net/news/75000-V-2017-godu-Nacbank-rasschityvaet-poluchit--chetyre-transha-ot-MVF',\n",
       "  'country': 'UA',\n",
       "  'domain_rank': 3864,\n",
       "  'title': 'В 2017 году Нацбанк рассчитывает получить четыре транша от МВФ',\n",
       "  'performance_score': 0,\n",
       "  'site': 'bigmir.net',\n",
       "  'participants_count': 0,\n",
       "  'title_full': 'В 2017 году Нацбанк рассчитывает получить четыре транша от МВФ',\n",
       "  'spam_score': 0.0,\n",
       "  'site_type': 'news',\n",
       "  'published': '2016-10-27T23:59:00.000+03:00',\n",
       "  'replies_count': 0,\n",
       "  'uuid': '7de061b1f81736fc573dad0938918666b594ee7a'},\n",
       " 'author': '',\n",
       " 'url': 'http://finance.bigmir.net/news/75000-V-2017-godu-Nacbank-rasschityvaet-poluchit--chetyre-transha-ot-MVF',\n",
       " 'ord_in_thread': 0,\n",
       " 'title': 'В 2017 году Нацбанк рассчитывает получить четыре транша от МВФ',\n",
       " 'locations': [],\n",
       " 'entities': {'persons': [], 'locations': [], 'organizations': []},\n",
       " 'highlightText': '',\n",
       " 'language': 'russian',\n",
       " 'persons': [],\n",
       " 'text': 'В 2017 году Нацбанк рассчитывает получить четыре транша от МВФ Сегодня, 18:59 Комментариев: 0 Сумма финансирования, которое Украина может получить в следующем году от МВФ, составляет 5,4 миллиарда. \\nНациональный банк Украины рассчитывает в 2017 году получить четыре транша кредита Международного валютного фонда на общую сумму $5,4 млрд. Об этом сообщил сегодня заместитель главы НБУ Дмитрий Сологуб, пишет УНИАН .\"В следующем году - четыре транша. Это стандартное расписание по программе МВФ\", - сказал Сологуб.Ранее стало известно, что Фонд допускает выделение Украине четвертого кредитного транша уже в этом году - в случае положительной оценки проводимых реформ.Визит миссии МВФ в Украину состоится в начале ноября. Liga.net Финансы',\n",
       " 'external_links': [],\n",
       " 'published': '2016-10-27T23:59:00.000+03:00',\n",
       " 'crawled': '2016-10-27T20:46:30.431+03:00',\n",
       " 'highlightTitle': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(json_files[0]) as f:\n",
    "    file = json.load(f)\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Однако, в этот раз нас будут интересовать только названия статей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filenames):\n",
    "    files = []\n",
    "    for file_name in filenames:\n",
    "        with open(file_name) as file:\n",
    "            file_dict = json.load(file)\n",
    "            files.append(file_dict)\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_features(files):\n",
    "    data = [{\n",
    "        'title': file['title'],\n",
    "        } for file in files]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dataset_to_df(files):\n",
    "    df = pd.DataFrame(files)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция, объединяющая все вышеперечисленное\n",
    "def get_files_df(filenames):\n",
    "    files = read_dataset(filenames)\n",
    "    dicts = get_features(files)\n",
    "    df = dataset_to_df(dicts)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы уменьшить нагрузку на память и увеличить скорость работы, будем использовать лишь часть датасета. Важно заметить, что выборка данных невелика и это еще скажется на качестве обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_files_df(json_files[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В 2017 году Нацбанк рассчитывает получить четы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Полиция устроила погоню со стрельбой за тракто...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Как прыгнуть с парашютом. Гройсман рассказал о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Из олимпийского Mercedes фигуристки Столбовой ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Родина взяла всего два гейма в матче с А. Радв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Гостья из будущего - летающая лампочка. Смотри...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Вопрос по выбору страниц для продвижения (прое...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>В Петушинском районе задержали закладчиков гер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Возбуждено уголовное дело после потери зрения ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Чувашия увеличила республиканский бюджет на 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  В 2017 году Нацбанк рассчитывает получить четы...\n",
       "1  Полиция устроила погоню со стрельбой за тракто...\n",
       "2  Как прыгнуть с парашютом. Гройсман рассказал о...\n",
       "3  Из олимпийского Mercedes фигуристки Столбовой ...\n",
       "4  Родина взяла всего два гейма в матче с А. Радв...\n",
       "5  Гостья из будущего - летающая лампочка. Смотри...\n",
       "6  Вопрос по выбору страниц для продвижения (прое...\n",
       "7  В Петушинском районе задержали закладчиков гер...\n",
       "8  Возбуждено уголовное дело после потери зрения ...\n",
       "9  Чувашия увеличила республиканский бюджет на 1,..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные получили, идем дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация (+ лемматизация)\n",
    "\n",
    "Любые текстовые данные стоит обязательно очищать. Я буду использовать word_tokenize из библеотеки nltk, уберу все стоп-слова, лишнюю пунктуацию и пробелы которые могут возникнуть как отдельные слова.\n",
    "\n",
    "Также, стоит учесть, что для дальнейшей обработки данных было бы неплохо провести лемматизацию текстов. Это обусловлено особенностями русского языка. Например, в английском языке эта мера была бы менее обязательной, но все же применимой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/temur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для лемматизации будет использовать библеотеку pymystem3. Это морфологический анализатор русского языка, используемый в Yandex (возможно я не прав). Как говорится, ссылочка в описании в конце ноутбука."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вы можете видеть функцию для препроцессинга текста. Возможно что-то не очень оптимизировано, но все же работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция токенизации текстов\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сказать',\n",
       " 'видеть',\n",
       " 'кто-то',\n",
       " 'наступать',\n",
       " 'грабли',\n",
       " 'разочаровывать',\n",
       " 'натравлять']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# # train, test = train_test_split(data_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообщем-то мы не особо нуждаемся в разделении датасета на train и test части, ибо я и так сокращал размер загружаемой выборки. А еще мы можем протестировать модель на некоторой части данных, на которых она уже будет обучена. Это не будет существенно влиять на результаты, тем более если нам их не с чем сравнивать :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Эмбеддинг\n",
    "\n",
    "Перейдем к самой требовательной в части объема памяти моменту. С сайта deeppavlov.ai загрузим предобученную модель на Wiki+Lenta FastText русского языка. Там было несколько моделей, я выбрал ту, у которой препроцессинг содержит \"tokenize (nltk word_tokenize), lemmatize (pymorphy2)\", так как я сам предобработал свои данные таким образом и это существенно уменьшает размер словаря модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/temur/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/temur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/temur/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/temur/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.fasttext_embedder import FasttextEmbedder\n",
    "from deeppavlov.models.embedders.tfidf_weighted_embedder import TfidfWeightedEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это путь к моей загруженной модели. Она будет лежать в той же директории, что и сам ноутбук. \n",
    "Ссылка для скачивания модели: http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_lemmatize/ft_native_300_ru_wiki_lenta_lemmatize.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_path = \"ft_native_300_ru_wiki_lenta_lemmatize.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-19 18:08:19.155 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `/home/temur/Documents/Kaggle/Internship Test - EORA/ft_native_300_ru_wiki_lenta_lemmatize.bin`]\n"
     ]
    }
   ],
   "source": [
    "fasttext_embedder = FasttextEmbedder(cap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Векторизатор для TF-IDF Weighted Эмбеддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры для векторизатора\n",
    "params = {}\n",
    "params['tokenizer'] = preprocess_text\n",
    "params['stop_words'] = russian_stopwords\n",
    "params['ngram_range'] = (1, 3)\n",
    "params['min_df'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вы можете видеть небольшой костыль, который мне пришлось создать чтобы пихнуть его в TfidfWeightedEmbedder. Я так сказать завернул мой векторизатор в оболочку с теми функциями которые необходимы для эмбеддера. На самом деле, правильней было бы унаследовать класс от deeppavlov.core.models.component.Component и инициализировать все, что нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(TfidfVectorizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = self\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        return self.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Vectorizer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vectorizer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit([i for i in data_df['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_tfidf = TfidfWeightedEmbedder(embedder=fasttext_embedder,\n",
    "        vectorizer=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11647623, -0.11186747,  0.07642709,  0.1264918 , -0.39655843,\n",
       "         0.06397477,  0.16238   ,  0.06901422,  0.01792966,  0.10028829,\n",
       "         0.16900687,  0.20173708, -0.09936717, -0.05134368, -0.16861708,\n",
       "         0.10392752, -0.06480429,  0.17310715,  0.18004686, -0.07454307,\n",
       "        -0.07744697,  0.1208401 , -0.11450679, -0.2510907 , -0.0626642 ,\n",
       "        -0.09429549,  0.14915459,  0.11352558,  0.09210832,  0.5100619 ,\n",
       "         0.2052138 ,  0.0385538 ,  0.05495359,  0.14050959, -0.09225752,\n",
       "         0.07475535,  0.06417041, -0.1401072 ,  0.10847577, -0.3682249 ,\n",
       "         0.21126395,  0.08137783, -0.04384461,  0.09862223, -0.02120385,\n",
       "        -0.09360278, -0.03669334, -0.05648015, -0.1173844 , -0.1363184 ,\n",
       "        -0.09581605,  0.19098663, -0.06546298,  0.31218368, -0.09501957,\n",
       "        -0.11795574, -0.1863031 , -0.17875998, -0.06008833,  0.00881153,\n",
       "         0.02532352, -0.05741346,  0.22504799, -0.35226208,  0.01570547,\n",
       "        -0.2264547 ,  0.02897066,  0.11514554, -0.03984465, -0.025241  ,\n",
       "         0.10626148, -0.01747205,  0.03211086, -0.00297699,  0.06985991,\n",
       "         0.1698071 , -0.05417434,  0.11860404, -0.01961716, -0.03564421,\n",
       "        -0.05408536,  0.07762437,  0.16577263, -0.1206978 , -0.01986543,\n",
       "         0.02297956, -0.12220976, -0.1236956 , -0.14771521, -0.53780323,\n",
       "        -0.12326792,  0.03913609,  0.28432205, -0.14784472,  0.23526254,\n",
       "        -0.0033827 ,  0.00805142,  0.0251464 ,  0.17742555,  0.04603014,\n",
       "         0.12714994,  0.08113527, -0.08791167, -0.29343557, -0.05225795,\n",
       "         0.04472775,  0.14845775, -0.08111256,  0.12161614, -0.11241059,\n",
       "        -0.03249026,  0.02996594, -0.0423204 , -0.13376486, -0.3364762 ,\n",
       "         0.14757952, -0.19590472, -0.08338656, -0.0882372 ,  0.06392416,\n",
       "         0.15267906, -0.28799537,  0.05749455, -0.16800368,  0.12343989,\n",
       "         0.00416379, -0.2099624 , -0.00937982, -0.08066132, -0.00622269,\n",
       "        -0.04901101, -0.03170589, -0.19228154,  0.0124902 , -0.09309403,\n",
       "        -0.1443635 , -0.0363134 ,  0.04904037,  0.00286583, -0.10177884,\n",
       "         0.13265966,  0.10407444, -0.09455481, -0.08016631, -0.0659066 ,\n",
       "         0.02910151,  0.35908222,  0.08603647,  0.03790467, -0.1939705 ,\n",
       "         0.17779616,  0.0560593 ,  0.15383556, -0.155813  , -0.15585247,\n",
       "         0.13400128, -0.0791578 , -0.21523125,  0.01374091, -0.10937496,\n",
       "         0.06447262,  0.0198696 ,  0.00748714, -0.1285019 ,  0.24582982,\n",
       "         0.12130509, -0.00351279, -0.16286273, -0.10789725,  0.05760616,\n",
       "         0.14392331, -0.02642216, -0.15844268, -0.17624886,  0.25665691,\n",
       "        -0.20048581, -0.15599342,  0.04837484,  0.0182846 ,  0.134184  ,\n",
       "        -0.00068445, -0.21450935, -0.11826396,  0.26410916, -0.10327177,\n",
       "         0.3400231 ,  0.06223378, -0.17059106, -0.17466003, -0.11059916,\n",
       "        -0.18503335, -0.09936221,  0.28312415, -0.08529051,  0.00899801,\n",
       "        -0.25721094, -0.00811873,  0.3954099 ,  0.31392205, -0.02561482,\n",
       "         0.15849838, -0.21036668, -0.32826033,  0.06668647,  0.06085814,\n",
       "         0.10597155,  0.08850536, -0.01722666,  0.11170515, -0.31643257,\n",
       "         0.12614973, -0.1417456 , -0.06819681, -0.03683779, -0.04159041,\n",
       "        -0.06754382, -0.04728982, -0.19782951,  0.13803557, -0.08456798,\n",
       "         0.14435194,  0.08772538,  0.09005363, -0.1875987 ,  0.03013959,\n",
       "         0.03978388, -0.01584234, -0.1616683 , -0.09058136,  0.08914763,\n",
       "         0.37265646,  0.12949334,  0.06235121, -0.06370501, -0.22340901,\n",
       "        -0.29162115,  0.06367455, -0.07528439,  0.01368769,  0.24018334,\n",
       "        -0.05408133,  0.1629366 ,  0.2061538 ,  0.20629093,  0.29439586,\n",
       "         0.10246915,  0.14386295,  0.34464678,  0.01579202,  0.03216264,\n",
       "         0.01982486,  0.21097453, -0.26849654,  0.03964762,  0.0131805 ,\n",
       "        -0.06835794, -0.04323324, -0.09566227,  0.24223906, -0.33569825,\n",
       "        -0.09594101, -0.21070008, -0.07037399, -0.41871026,  0.14469352,\n",
       "         0.21846674, -0.31543493, -0.00440793, -0.05341817, -0.1782163 ,\n",
       "         0.22651424,  0.10672951, -0.17123942,  0.10116002, -0.17583278,\n",
       "        -0.06908369,  0.04375463,  0.1781471 , -0.07645679, -0.06289198,\n",
       "         0.4038928 ,  0.10220998, -0.2054466 ,  0.19364119,  0.279633  ,\n",
       "         0.01891782, -0.01838247,  0.00380962, -0.04196749,  0.15142053,\n",
       "        -0.04514993, -0.18749152,  0.3791477 ,  0.04682894, -0.22421305,\n",
       "         0.22875535, -0.15312447, -0.03779873, -0.09073417,  0.10273034]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_tfidf([['большой фильм']])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш FastText эмбеддер для предложений готов!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель для построения пространства и нахождения соседей\n",
    "\n",
    "Здесь можно было бы использовать KMeans классификатор и юзать его для нахождения соседей векторов. Однако, я решил, что можно добавить небольшое улучшение.\n",
    "\n",
    "Я решил использовать алгоритм Approximate Nearest Neighbor, который, как понятно из названия, аппроксимирует результат и может не выдать точные результаты ближайших векторов. Однако! Он работает быстрей и требует меньше памяти за счет применения LSH (locality-sensitive hashing) и BBF (best bin first) алгоритмов.\n",
    "\n",
    "На мой вгляд, данные преимущества (скорость и меньшее подтребление памяти) будут очень кстати для больших объемов данных, что весьма вероятно в данной задачи.\n",
    "\n",
    "Я нашел удобный фрейворк NearPy, который реализует данный алгоритм, предлагая удобный функционал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nearpy import Engine\n",
    "from nearpy.hashes import RandomBinaryProjections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размерность наших векторов (взято с вывода эмбеддинга)\n",
    "dimension = 300\n",
    "\n",
    "# Создадим рандомный бинарный хеш с 2 битами\n",
    "rbp = RandomBinaryProjections('rbp', 2)\n",
    "\n",
    "# создадим engine\n",
    "engine = Engine(dimension, lshashes=[rbp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле бинарный хеш сильно влияет на конечный результат. Я заметил, что чем больше количество битов, тем хуже получаются результаты и, видимо, больше векторов попадает в одну аппроксимированную кучу. Поэтому я уменьшил количество битов, но конечно это сказывается на том, что прироста в производительности мы не получим. Однако, при большем количестве данных, мы могли бы себе позволить увеличить кол-во битов (точнее вынуждены были бы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция добавления векторов в engine\n",
    "def store_vectors(df, engine, feature='title'):\n",
    "    for index, row in df.iterrows():\n",
    "        tfidf_vec = fasttext_tfidf([[row[feature]]])[0]\n",
    "        vector = tfidf_vec.reshape((dimension,))\n",
    "        engine.store_vector(vector, index)\n",
    "\n",
    "# добавим вектора\n",
    "store_vectors(data_df, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# функция для считывания данных (в данном случае названий статей) по выводу engine\n",
    "def get_samples(df, neighbors, feature='title'):\n",
    "    samples = []\n",
    "    for neighbor in neighbors:\n",
    "        vec, index, res = neighbor\n",
    "        value = df.iloc[index][feature]\n",
    "        samples.append(value)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для запрашивания ближайших соседей данного названия (с необходимой предобработкой)\n",
    "def query(engine, sample, samples_only=True):\n",
    "    tfidf_vec = fasttext_tfidf([[sample]])[0]\n",
    "    vector = tfidf_vec.reshape((dimension,))\n",
    "    neighbors = engine.neighbours(vector)\n",
    "    if samples_only:\n",
    "        samples = get_samples(data_df, neighbors)\n",
    "        return samples\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# достанем небольшую выборку, чтобы потестить на ней модель\n",
    "rand_samples = data_df.sample(n=5, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос статьи: В Сочи автомобиль протаранил остановку с людьми, двое погибли\n",
      "Похожая статья: В Сочи автомобиль протаранил остановку с людьми, двое погибли\n",
      "Похожая статья: В США пьяная водитель внедорожника протаранила автобус \"Свидетелей Иеговы\"\n",
      "Похожая статья: Грузовик протаранил рейсовый автобус с пассажирами: один человек погиб, четверо ранены\n",
      "Похожая статья: Могилев: маршрутка столкнулась с легковым автомобилем, тот перевернулся\n",
      "Похожая статья: В центре Омска водитель джипа потерял сознание и протаранил машины\n",
      "Похожая статья: В Омске пассажиры проломили голову водителю и угнали его автомобиль\n",
      "Похожая статья: На юге Москвы автомобиль въехал в автобусную остановку\n",
      "Похожая статья: Два человека погибли при столкновении автомобиля с остановкой в Москве\n",
      "Похожая статья: Автомобиль сбил четырёх человек на остановке в Москве, двое погибли\n",
      "Похожая статья: В Ростове автомойщик перепутал педали и протаранил дом иномаркой клиента\n",
      "==================================================\n",
      "Запрос статьи: Сатирические рисунки, которые заставят вас улыбнуться\n",
      "Похожая статья: Сатирические рисунки, которые заставят вас улыбнуться\n",
      "Похожая статья: Премьера песни Эмина Агаларова и Ани Лорак о безответной любви \"Я не могу сказать\": артисты рассказали SPLETNIK.RU о новой работе\n",
      "Похожая статья: \"У украинцев нет права войти в НАТО -- русские этого не потерпят\" // Американский политолог Джон Миршаймер рассказал \"Ъ\" о проблемах, разделивших Россию и Запад\n",
      "Похожая статья: Соколов: не знаю, почему на первый период «Югра» вышла скованной\n",
      "Похожая статья: Отменная подборка веселых рисунков и комиксов\n",
      "Похожая статья: Алексей Кудашов: «Немного опасались за физическое состояние игроков, поскольку только пару дней назад вернулись из очень сложного и долгого путешествия»\n",
      "Похожая статья: \"Это станция Гаага, мы заждались Вас уже\": Михаил Ефремов жестко высмеял Путина в новом хите\n",
      "Похожая статья: Вы только полюбуйтесь ее красотой! Российская модель Мария Лиман в откровенной фотосессии\n",
      "Похожая статья: WikiLeaks опубликовал четвертую часть писем главы штаба Клинтон\n",
      "Похожая статья: Актер Ефремов высмеял Путина: Станция Гаага, мы заждались Вас уже\n",
      "==================================================\n",
      "Запрос статьи: Секретные документы Пентагона стали добычей китайской разведки\n",
      "Похожая статья: Секретные документы Пентагона стали добычей китайской разведки\n",
      "Похожая статья: СМИ узнали о краже китайской разведкой секретных планов Пентагона\n",
      "Похожая статья: В Генштабе рассказали о работе российской разведки в Сирии\n",
      "Похожая статья: Боевики намерены отвести войска в районе Станицы Луганской 2-3 октября, - разведка\n",
      "Похожая статья: Боевики «засветили» на Донбассе российскую станцию наземной разведки\n",
      "Похожая статья: «Новый Сноуден» похитил информацию об американских разведчиках-нелегалах\n",
      "Похожая статья: ВВС Индонезии провели крупные военные учения в Южно-Китайском море\n",
      "Похожая статья: Подрядчик АНБ арестован в США по обвинению в краже секретных документов\n",
      "Похожая статья: В Минобороны прокомментировали сообщения американской разведки о переброске «Искандеров» под Калининград\n",
      "Похожая статья: СМИ: Оборонные предприятия России получали китайскую технику под видом отечественной\n",
      "==================================================\n",
      "Запрос статьи: Горди Дуайер: «Удаление Тейлора стало ключевым моментом в матче с ЦСКА»\n",
      "Похожая статья: Горди Дуайер: «Удаление Тейлора стало ключевым моментом в матче с ЦСКА»\n",
      "Похожая статья: Почеттино: «Тоттенхэм» ждут 7 матчей за 23 дня — это невероятно важный отрезок\n",
      "Похожая статья: Алексей Кудашов: «На «Локомотиве» большое психологическое давление, потеряли уверенность из-за того, что не можем забить, не можем выиграть»\n",
      "Похожая статья: Норвежские лыжники пользуются небулайзерами в вакс-трейлере для приема медикаментов во время соревнований\n",
      "Похожая статья: Главный тренер «Урала»: высокий класс футболистов «Зенита» сказался в реализации моментов\n",
      "Похожая статья: Кубок чемпионов. Матч «Расинга» и «Манстера» перенесен из-за смерти главного тренера ирландцев\n",
      "Похожая статья: Вратарь «Трактора» Францоуз впервые сыграл на ноль на уровне КХЛ\n",
      "Похожая статья: Тренер «Сада Крузейро» — о победе над «Зенитом»: это был идеальный матч\n",
      "Похожая статья: «Салават Юлаев» – «Автомобилист». Вратарь гостей Сохатский пропустил 3 гола в первом периоде и был заменен\n",
      "Похожая статья: Два отбитых пенальти Стекеленбург в матче против «Сити» – восьмой случай в истории АПЛ\n",
      "==================================================\n",
      "Запрос статьи: Российский дайвер погиб в Таиланде – СМИ\n",
      "Похожая статья: Российский дайвер погиб в Таиланде – СМИ\n",
      "Похожая статья: В Таиланде во время прыжка с парашютом разбился российский спортсмен\n",
      "Похожая статья: Россиянин погиб в Таиланде\n",
      "Похожая статья: Россиянина обвинили в нападении с ножом на соотечественницу в Таиланде\n",
      "Похожая статья: Меркель и Обама осудили российские \"варварские\" авиаудары по Алеппо - BBC Україна\n",
      "Похожая статья: Сколько мирного населения погибло из-за российских действий в Сирии\n",
      "Похожая статья: Российские наемники понесли рекордные потери на Донбассе\n",
      "Похожая статья: 15-летняя спортсменка из Ирландии обвинила российского тренера в нападении\n",
      "Похожая статья: Курс юаня к доллару упал до шестилетнего минимума\n",
      "Похожая статья: СМИ: Американский суд вызвал кувейтца на слушание через «Твиттер»\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# выведим результаты для выборки\n",
    "for index, row in rand_samples.iterrows():\n",
    "    title = row['title']\n",
    "    neighbors = query(engine, title)\n",
    "    print('Запрос статьи:', title)\n",
    "    for sample in neighbors:\n",
    "        print('Похожая статья:', sample)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не обращайте внимание на то, что первые значения те же самые, что сами запросы. Просто они уже присутствуют в системе. Давайте попробуем дать запросы несуществующих в engine статей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "С середины недели в Петербурге пойдет мокрый снег\n",
      "Министр Силуанов пообещал, что цены на водку сильно не изменятся - Бизнес - Новости Санкт-Петербурга - Фонтанка.Ру\n",
      "На территории Кировского завода в Петербурге произошел пожар\n",
      "В Санкт-Петербурге школьница впала в кому на стадионе\n",
      "Синоптики: штормовой ветер сохранится\n",
      "На востоке Петербурга дотла сгорело офисное здание\n",
      "В Петербурге встретятся лидеры газовой отрасли\n",
      "Сильный туман в Петербурге увеличил число ДТП, но не повлиял на «Пулково»\n",
      "Юг Петербурга затопило кипятком из лопнувшей трубы\n",
      "Пассажиры задержанного «Сапсана» прибудут в Петербург с опозданием в три часа\n"
     ]
    }
   ],
   "source": [
    "results = query(engine, 'В среду в Петербурге сохранится сухая погода')\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Режим зеркала. Мы посмотрели, как поживает первая в стране интерактивная остановка спустя месяц после открытия (15 фото)\n",
      "Граффити с изображением лидера донецких сепаратистов Арсения Павлова по прозвищу Моторола появилось в Петербурге\n",
      "Комментарии - Россия наложила вето на проект резолюции об установлении бесполетной зоны над Алеппо - Delfi\n",
      "Украина стала ассоциированным членом Европейской организации ядерных исследований\n",
      "Эксперт: Борьба с кибератаками возможна при совместных усилиях РФ и США\n",
      "В Пентагоне заявили, что примут меры в связи с размещением С-300 в Сирии\n",
      "C новым стандартом IEEE скорость интернета в обычных кабелельных сетях вырастет в 5 раз\n",
      "Открытие студии мобильной разработки «с нуля» в Питере — 3.5 года спустя. Реинкарнация. Часть 2 / Блог компании tapki.com\n",
      "Норвежские лыжники пользуются небулайзерами в вакс-трейлере для приема медикаментов во время соревнований\n",
      "«Южмаш» пообещал отгрузить ракету-носитель «Зенит» для «Морского старта» в 2017 году\n"
     ]
    }
   ],
   "source": [
    "results = query(engine, 'Политика: У США нет оснований изображать себя победителем в войне с террором')\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фаната из Тулы будут судить за файер на матче с «Зенитом»\n",
      "Мендес: игра с «Зенитом» стала хорошей проверкой для «Сада Крузейро»\n",
      "Премию Гавела присудили экс-пленнице \"ИД\", над которой жестоко издевались боевики\n",
      "Федецкий признался, что ему тяжело смотреть за матчем сборной Украины со стороны\n",
      "Тренер «Сада Крузейро» — о победе над «Зенитом»: это был идеальный матч\n",
      "СМИ: Абрамович хочет купить защитника «Ювентуса» за рекордную сумму\n",
      "Источник: матч «Зенит» — «Спартак» изначально должен был судить другой арбитр\n",
      "«Он — человек ближайших окрестностей Земли»: Астроном Владимир Сурдин вспоминает своего коллегу Клима Чурюмова, который открыл самую знаменитую комету — Meduza\n",
      "Гимаев: было бы замечательно увидеть в «Сент-Луисе» связку Тарасенко — Якупов\n",
      "Трамп и Клинтон не пожали друг другу руки перед началом финальных дебатов\n"
     ]
    }
   ],
   "source": [
    "results = query(engine, 'Фаната из Тулы будут судить за файер на матче с «Зенитом»')\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы можем видеть по результам. Предсказания часто очень похожи на запрашиваемые статьи. Однако, есть статьи которые не очень подходят по смыслу и все же нужно признать, что алгоритм может быть улучшен. Ну, всегда есть к чему стремится!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коротко о том, что хотелось бы улучшить в данном решении и вообще в решении задачи в целом:\n",
    "\n",
    "- взять весь датасет, а не только часть;\n",
    "- вместо тайтлов брать сами тексты, так как они содержать больше информации о том, о чем статья;\n",
    "- использовать ELMo эмбеддинг, так как он context-sensitive и по сути является state-of-the-art решением (не считая Bert) для эмбеддинга;\n",
    "- использовать иную модель для кластеризации. Возможно применение convolutional NN;\n",
    "- хотелось бы оценить работу по каким-либо метрикам. Однако, это может оказаться задачей не из легких в данной задаче. Но хотя бы можно было бы взять скоростную метрику как один из вариантов.\n",
    "- вообще было бы хорошо использовать и иные признаки, такие как время публикации, популярность в соцсетях и сам сайт. Из личного опыта, могу сказать, что последняя идея (на счет сайта) мне не нравится, ибо, к примеру, Google Chrome предлагает мне постоянно статьи с сайтов, которые я когда-то посещал, но больше не интересуюсь ими и это слегка раздражает меня, как пользователя.\n",
    "\n",
    "Чтож, здесь на этом все. Теперь мы можем перейти к небольшой имплементации системы рекомендаций сайтов, которая реализована в папке ./system, также есть небольшой скрипт для демонстрации - файл test_system.py.\n",
    "\n",
    "Хочу также отметить, что задача состояла в том, что у пользователя есть история предыдущих статей. Здесь, мы показали пример, когда у пользователя 1 статья. Чтобы масштабировать, я решил что буду предлагать пользователю до 10 статей, каждая из которых равновероятно может подходить одной из статей, прочитанных пользователем. Также, я ограничу историю статей пользователя до 10 статей, ибо не хотелось бы учитывать слишком старые статьи для рекомендаций. Все это реализовано в ранее указанных директории и файле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- PyMystem3: https://github.com/nlpub/pymystem3\n",
    "- Препроцессинг русского текста: https://www.kaggle.com/alxmamaev/how-to-easy-preprocess-russian-text\n",
    "- NearPy фреймворк: https://github.com/pixelogik/NearPy\n",
    "- Визуализация данных с Plotly: https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "- Эмбеддеры из DeepPavlov: http://docs.deeppavlov.ai/en/latest/apiref/models/embedders.html\n",
    "- Предобученные модели эмбеддинга для русского языка: http://docs.deeppavlov.ai/en/latest/intro/pretrained_vectors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
